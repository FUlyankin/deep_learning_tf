# Алгоритм обратного распространения ошибки

- [Скачать себе презентацию](https://github.com/FUlyankin/deep_learning_tf/raw/main/week02_backprop/nn_slides_2.pdf)


## Задание 

Прорешать [листочек про градиентный спуск.](https://github.com/FUlyankin/deep_learning_tf/raw/main/week02_backprop/neural_nets_tasks_part_3.pdf) На следующем семинаре будет разбор задачек, которые вы не смогли решить. На четвёртой паре будет проверочная. На следующей паре проверочная по 1-2 листочкам. 


## Что можно почитать и посмотреть

* [Визуализация Momentum.](https://distill.pub/2017/momentum/) У distill.pub очень качественные статьи. Рекомендую прочитать весь сайт. Ребята выгорели и больше ничего не публикуют :(
* [Семинар про SGD на ПМИ ФКН,](https://github.com/esokolov/ml-course-hse/blob/master/2021-fall/seminars/sem03-gd.ipynb) а также [конспект лекции.](https://github.com/esokolov/ml-course-hse/blob/master/2021-fall/lecture-notes/lecture03-linregr.pdf) 


__Николенко:__ глава 3, глава 4.4, 4.5

__Глубокое обучение Гудфелоу:__ 
- 6.5 про алгоритм обратного распространение ошибки (написано очень сложнымм языком) 
- Глава 8 оптимизация в обучении глубоких моделей (много дополнительной информации про оптимизацию)


## Откуда я воровал материалы

* Идею вставить в презу свою фотку с гор я [спёр у Юры Кашницкого.](https://habr.com/ru/company/ods/blog/326418/)
* Часть слайдов про градиентный спуск нагло [украл у Жени Соколова с курса ИАД.](https://github.com/hse-ds/iad-deep-learning/blob/master/2021/lectures/lecture04-convnets-optimization.pdf) Лекцию можно найти [в плейлисте ИАД](https://www.youtube.com/playlist?list=PLEwK9wdS5g0qa3PIhR6HBDJD_QnrfP8Ei)
* Слайды про перемешивание взяты из [курса НЛП Лены Войты](https://github.com/yandexdataschool/nlp_course/tree/2019/week01_embeddings)

