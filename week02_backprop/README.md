# От регрессии к нейросети

лекция: градиентный спуск, алгоритм обратного распространения ошибки 
семинар: разбираем листочки 1-2 с задачками 


## Задание: 

- Прорешать [листочек про градиентный спуск.]( ) На следующем семинаре будет разбор задачек, которые вы не смогли решить. На четвёртой паре будет проверочная. На следующей паре проверочная по 1-2 листочкам. 


## Почиташки

* [Видос с бэкпропом за 10 минут (eng)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
* [Андрей Карпатый читает лекцию о backprop (eng),](https://www.youtube.com/watch?v=59Hbtz7XgjM) и пишет о нём [в посте (eng)](http://cs231n.github.io/optimization-2/)
* [Лекция Андрея Зимовнова про алгоритм обратного распространения ошибки с курсеры (eng)](https://www.coursera.org/lecture/intro-to-deep-learning/backpropagation-CxUe5)


## Откуда я всё спёр: 

* Схему с объяснением бэкпропа я [взял у Андрея Зимовнова,](https://github.com/ZEMUSHKA/mml-minor) а затем дополнил и улучшил. В Tikz я её перерерисовывать упоролся.
* Идею вставить в презу свою фотку с гор я [спёр у Юры Кашницкого.](https://habr.com/ru/company/ods/blog/326418/)

