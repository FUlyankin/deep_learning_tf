{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning \n",
    "\n",
    "Transfer learning это когда ты берёшь чужую модель и адаптируешь её под свою задачу. В этой тетрадке будет два примера transfer learning. Для эмбедингов и для картинок. \n",
    "\n",
    "\n",
    "##  Про tensorhub\n",
    "\n",
    "Каждый раз, обучая нейронку, мы сначала рандомно инициализируем веса, а после в ходе бэкпропа обучаем модель. Если мы сразу же угадываем хорошие веса, модель сходится быстрее. Иногда можно брать в качестве инициализации веса, полученные другими исследователями и на их основе дообучать модель под свой выход. Это здорово упрощает задачу обучения и экономит многие часы работы. Для такого дележа в $2017$ году была придумана платформа __TensorFlow Hub.__ \n",
    "\n",
    "Сегодня с помощью этой платформы разработчики делятся друг с другом уже готовыми предобученными весами. Для работы библиотеки нужна версия tensorflow выше `1.7`. Инструкцию по установке можно найти на [сайте tensorflow.](https://www.tensorflow.org/hub/installation)\n",
    "\n",
    "В случае проблем с установкой, повысить версию tensorflow до актуальной помогает команда:\n",
    "\n",
    "```python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ELMO-эмбединги.\n",
    "\n",
    "\n",
    "[ ](https://simkl.in/fanart/17/17052285f7e1391f7_0.jpg)\n",
    "\n",
    "Пробуем воспользоваться хабом. Скачаем [модель от IPavlov с эмбедингами для русскоязычных новостей.](http://docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html) Примеры использования хаба с моделями IPavlov можно найти [в документации.](http://docs.deeppavlov.ai/en/latest/apiref/models/embedders.html#deeppavlov.models.embedders.elmo_embedder.ELMoEmbedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/r2/6lthpk110g7d1kjwlgx6p7100000gn/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz'.\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 30.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 60.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 90.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 120.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 150.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 180.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 210.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 240.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 270.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 300.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 330.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 511.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 531.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 561.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 591.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 621.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 651.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 681.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 711.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 741.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 771.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 801.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 831.06MB\n",
      "INFO:tensorflow:Downloaded http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz, Total size: 858.71MB\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz'.\n"
     ]
    }
   ],
   "source": [
    "# подгружаем модель\n",
    "elmo = hub.Module(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем сессию в tensorflow и применяем к чему-нибудь предобученную нейросетку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.05817385,  0.22493355, -0.1920293 , ..., -0.14448947,\n",
       "         -0.1242556 ,  1.0148408 ],\n",
       "        [ 0.5359629 ,  0.28685376,  0.28028587, ..., -0.08028417,\n",
       "          0.4908908 ,  0.75939935]],\n",
       "\n",
       "       [[ 0.34336394,  1.0031183 , -0.15972564, ...,  1.2442503 ,\n",
       "          0.6102935 ,  0.43388352],\n",
       "        [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n",
       "         -0.0066415 , -0.01344293]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "embeddings = elmo([\"это предложение\", \"word\"], signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также hub поддерживает токенизированный формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.60400033, -0.16130012,  0.5647884 , ..., -0.00376102,\n",
       "         -0.0382006 ,  0.26321295],\n",
       "        [ 0.01834123,  0.17055827,  0.5311495 , ..., -0.56755346,\n",
       "          0.62669814, -0.05939047],\n",
       "        [ 0.32425952,  0.17909637,  0.01657113, ...,  0.1866094 ,\n",
       "          0.7392498 ,  0.08285775]],\n",
       "\n",
       "       [[ 1.1322286 ,  0.19077665, -0.17811388, ...,  0.42973173,\n",
       "          0.23391487, -0.01294377],\n",
       "        [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n",
       "         -0.0066415 , -0.01344293],\n",
       "        [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n",
       "         -0.0066415 , -0.01344293]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_input = [[\"мама\", \"мыла\", \"раму\"], [\"рама\", \"\", \"\"]]\n",
    "tokens_length = [3, 1]\n",
    "embeddings = elmo(\n",
    "    inputs={\n",
    "            \"tokens\": tokens_input,\n",
    "            \"sequence_len\": tokens_length\n",
    "            },\n",
    "    signature=\"tokens\",\n",
    "    as_dict=True)[\"elmo\"]\n",
    "\n",
    "sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(1024)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме новостей в [документации проекта можно найти](http://docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html) википедию, русскоязычный твиттер и многие другие эмбединги. Там же можно найти довольно много полноценных классификаторов. \n",
    "\n",
    "Чуть позже мы попробуем впихнуть предобученный embeddig слой в нашу собственную нейросетку."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
