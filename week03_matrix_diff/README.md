# Матричные производные и алгоритм обратного распространения ошибки

- __Внимание!__ На этой паре будет quiz-1 "всего лишь функция" по 1-2 листочкам!

- [Скачать себе презентацию](https://github.com/FUlyankin/deep_learning_tf/raw/main/week02_backprop/nn_slides_3.pdf)

- Скачать себе [конспект лекции про производные.](https://github.com/FUlyankin/deep_learning_tf/raw/main/week03_matrix_diff/sem03-vector-diff.pdf) Я частично делал его для курса [МО-1 на ФКН.](https://github.com/esokolov/ml-course-hse) Если хочется можно [посмотреть его запись.](https://youtu.be/jNJrzuJm59k)

- Тетрадка для семинара в колабе: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FUlyankin/deep_learning_tf/blob/main/week01_intro/sgd_experiments/SGD_experiments.ipynb)


## Задание 

- На следующей паре будет quiz-2 "Градиентный спуск без матриц". Через пару будет проверочная по матричным производным.
- Порешать [листочек про матричные производные](https://github.com/FUlyankin/deep_learning_tf/raw/main/week03_matrix_diff/neural_nets_tasks_part_4.pdf) и [листочек про обратное распространеие ошибки](https://github.com/FUlyankin/deep_learning_tf/raw/main/week03_matrix_diff/neural_nets_tasks_part_5.pdf) На следующем семинаре разберём задачи, которые попросите. 


## Что можно почитать и посмотреть

- [Книга про матричные производные](http://matrixcookbook.com/)
- [Бесплатный калькулятор матричных производных](http://www.matrixcalculus.org/)

- [Бэкпроп по шагам](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
- На следующем семинаре мы будем подробно разбирать алгоритм обратного распространения на задачках. Можно попробовать [порешать на него задачи](https://github.com/FUlyankin/deep_learning_tf/raw/main/week03_matrix_diff/neural_nets_tasks_part_5.pdf)
- [Видос с бэкпропом за 10 минут (eng)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
- [Андрей Карпатый читает лекцию о backprop (eng),](https://www.youtube.com/watch?v=59Hbtz7XgjM) и пишет о нём [в посте (eng)](http://cs231n.github.io/optimization-2/)
- [Лекция Андрея Зимовнова про алгоритм обратного распространения ошибки с курсеры (eng)](https://www.coursera.org/lecture/intro-to-deep-learning/backpropagation-CxUe5)


## Откуда я воровал материалы

- Идею схемы с объяснением бэкпропа я [взял у Андрея Зимовнова,](https://github.com/ZEMUSHKA/mml-minor) а затем дополнил. В Tikz я её перерерисовывать упоролся.
- Конспкт про матричные производные основан [конспекте с методов оптимизации](https://drive.google.com/file/d/1bNWF262guJHTphptCTYARAsUKRPC9QmS/view

